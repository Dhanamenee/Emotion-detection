{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4+rYzXlRNvO99gUKMX/mK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dk_xly8TsMLZ","executionInfo":{"status":"ok","timestamp":1750181942963,"user_tz":-330,"elapsed":13911,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"e95eb6f1-78ba-49b0-d0d1-3c5d9839302e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"]}],"source":["!pip install opencv-python"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0KX7vygse50","executionInfo":{"status":"ok","timestamp":1750181949631,"user_tz":-330,"elapsed":4185,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"c76b2b11-ac0f-4b18-8fd0-e0643989ad3a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["pip install streamlit opencv-python tensorflow torch torchvision dlib mediapipe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSiyC25Ossde","executionInfo":{"status":"ok","timestamp":1750181963870,"user_tz":-330,"elapsed":10439,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"4406cf85-0581-4bfa-c955-197ae8b07e11"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.6)\n","Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.8)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n","Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.42.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.58.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","source":["import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","#import matplotlib.pyplot as plt\n","import os\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import cv2\n","import matplotlib.pyplot as plt\n","from torch import optim\n","from torch.autograd import Variable\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xtc7SIfs4s-","executionInfo":{"status":"ok","timestamp":1750181990835,"user_tz":-330,"elapsed":15396,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"b8f039b8-f5b8-48b6-8483-2f06fb83f489"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"]}]},{"cell_type":"code","source":["transform  = transforms.Compose([\n","    transforms.Resize((48, 48)),\n","    transforms.ToTensor()\n","])\n","data_dir='/content/drive/MyDrive/test/training'\n","train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","class EmotionClassifier(nn.Module):\n","    def __init__(self):\n","        super(EmotionClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128 * 1 * 1, 256)\n","        self.fc2 = nn.Linear(256, 7)\n","\n","    def forward(self, X):\n","        X = self.conv1(X)\n","        X = self.conv2(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv3(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv4(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv5(X)\n","        X = torch.relu(self.pool(X))\n","        X = torch.flatten(X, 1)\n","        X = self.fc1(X)\n","        X = self.dropout(X)\n","        X = self.fc2(X)\n","        return X\n","\n","model = EmotionClassifier()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","n_epoch = 40\n","for epoch in range(1, n_epoch + 1):\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    train_true = []\n","    train_pred = []\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        total += labels.size(0)\n","        _, predictions = torch.max(outputs, 1)\n","        correct += (predictions == labels).sum().item()\n","        train_true.extend(labels.cpu().numpy())\n","        train_pred.extend(predictions.cpu().numpy())\n","\n","    print(f\"Epoch: {epoch}/{n_epoch} Loss: {running_loss / len(train_loader)}\")\n","    print(f\"Epoch: {epoch}/{n_epoch} Accuracy: {correct / total}\")\n","print('Training my data')\n","acc = accuracy_score(train_true, train_pred)\n","f1 = f1_score(train_true, train_pred, average='weighted')\n","epoch_loss = running_loss / len(train_loader)\n","\n","print(f\"Epoch {epoch}/{n_epoch}\")\n","print(f\"Train Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n","print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRF32tqhtr1t","executionInfo":{"status":"ok","timestamp":1750208620479,"user_tz":-330,"elapsed":26610882,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"385d8110-acb6-403d-9095-9082333e14c4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/40 Loss: 1.8114573868212769\n","Epoch: 1/40 Accuracy: 0.2476309486618765\n","Epoch: 2/40 Loss: 1.7085645553406812\n","Epoch: 2/40 Accuracy: 0.3048005831510986\n","Epoch: 3/40 Loss: 1.5581126582212375\n","Epoch: 3/40 Accuracy: 0.3902599881981325\n","Epoch: 4/40 Loss: 1.4549454828345947\n","Epoch: 4/40 Accuracy: 0.43979312020549133\n","Epoch: 5/40 Loss: 1.3755931702755664\n","Epoch: 5/40 Accuracy: 0.47193585337915234\n","Epoch: 6/40 Loss: 1.3102057983687398\n","Epoch: 6/40 Accuracy: 0.5009892741851505\n","Epoch: 7/40 Loss: 1.256137533272543\n","Epoch: 7/40 Accuracy: 0.5256690617515359\n","Epoch: 8/40 Loss: 1.2099603904074225\n","Epoch: 8/40 Accuracy: 0.5437883994585025\n","Epoch: 9/40 Loss: 1.1692748205378636\n","Epoch: 9/40 Accuracy: 0.5589225589225589\n","Epoch: 10/40 Loss: 1.133723197845984\n","Epoch: 10/40 Accuracy: 0.5757575757575758\n","Epoch: 11/40 Loss: 1.0941432182585096\n","Epoch: 11/40 Accuracy: 0.5888784754764136\n","Epoch: 12/40 Loss: 1.05698748083411\n","Epoch: 12/40 Accuracy: 0.604568016939151\n","Epoch: 13/40 Loss: 1.02096843183636\n","Epoch: 13/40 Accuracy: 0.6185914124058454\n","Epoch: 14/40 Loss: 0.9840779283996692\n","Epoch: 14/40 Accuracy: 0.6298726092540525\n","Epoch: 15/40 Loss: 0.9468204056581567\n","Epoch: 15/40 Accuracy: 0.6479919469610191\n","Epoch: 16/40 Loss: 0.9093934422923245\n","Epoch: 16/40 Accuracy: 0.6617029400534555\n","Epoch: 17/40 Loss: 0.8711380493164592\n","Epoch: 17/40 Accuracy: 0.6737824985247666\n","Epoch: 18/40 Loss: 0.8362398999975735\n","Epoch: 18/40 Accuracy: 0.6886389669894825\n","Epoch: 19/40 Loss: 0.7999085977458531\n","Epoch: 19/40 Accuracy: 0.7019334235829081\n","Epoch: 20/40 Loss: 0.7615451415820867\n","Epoch: 20/40 Accuracy: 0.7199139157902045\n","Epoch: 21/40 Loss: 0.7184504879169273\n","Epoch: 21/40 Accuracy: 0.7351869207539311\n","Epoch: 22/40 Loss: 0.6811923798128185\n","Epoch: 22/40 Accuracy: 0.7487243569717796\n","Epoch: 23/40 Loss: 0.6429132638244862\n","Epoch: 23/40 Accuracy: 0.7645874553091048\n","Epoch: 24/40 Loss: 0.6031280052615589\n","Epoch: 24/40 Accuracy: 0.7786455621507168\n","Epoch: 25/40 Loss: 0.5667181318495568\n","Epoch: 25/40 Accuracy: 0.7939185671144434\n","Epoch: 26/40 Loss: 0.5323418598791603\n","Epoch: 26/40 Accuracy: 0.805165052587733\n","Epoch: 27/40 Loss: 0.4978810157482156\n","Epoch: 27/40 Accuracy: 0.8190843139296747\n","Epoch: 28/40 Loss: 0.46169343734621077\n","Epoch: 28/40 Accuracy: 0.8319275226491721\n","Epoch: 29/40 Loss: 0.42122137690423994\n","Epoch: 29/40 Accuracy: 0.8477906209864973\n","Epoch: 30/40 Loss: 0.3944494156011863\n","Epoch: 30/40 Accuracy: 0.8597660453330557\n","Epoch: 31/40 Loss: 0.36414694683533266\n","Epoch: 31/40 Accuracy: 0.8662223610677219\n","Epoch: 32/40 Loss: 0.3365641884447864\n","Epoch: 32/40 Accuracy: 0.8795862404109827\n","Epoch: 33/40 Loss: 0.30550557655281285\n","Epoch: 33/40 Accuracy: 0.8913533965080357\n","Epoch: 34/40 Loss: 0.2793796896587201\n","Epoch: 34/40 Accuracy: 0.9009684473602\n","Epoch: 35/40 Loss: 0.25513753003867695\n","Epoch: 35/40 Accuracy: 0.9077024540942067\n","Epoch: 36/40 Loss: 0.23539771469977666\n","Epoch: 36/40 Accuracy: 0.9161720295740914\n","Epoch: 37/40 Loss: 0.21388791489531675\n","Epoch: 37/40 Accuracy: 0.9251969870526572\n","Epoch: 38/40 Loss: 0.20577622501471066\n","Epoch: 38/40 Accuracy: 0.9283557221701552\n","Epoch: 39/40 Loss: 0.1826518252926086\n","Epoch: 39/40 Accuracy: 0.9360616474018536\n","Epoch: 40/40 Loss: 0.17022529214240206\n","Epoch: 40/40 Accuracy: 0.9403658578916311\n","Training my data\n","Epoch 40/40\n","Train Loss: 0.1702 | Acc: 0.9404 | F1: 0.9403\n","LR: 1.00e-04\n"]}]},{"cell_type":"code","source":["transform  = transforms.Compose([\n","    transforms.Resize((48, 48)),\n","    transforms.ToTensor()\n","])\n","data_dir='/content/drive/MyDrive/test/training'\n","train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","class EmotionClassifier(nn.Module):\n","    def __init__(self):\n","        super(EmotionClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128 * 1 * 1, 256)\n","        self.fc2 = nn.Linear(256, 7)\n","\n","    def forward(self, X):\n","        X = self.conv1(X)\n","        X = self.conv2(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv3(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv4(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv5(X)\n","        X = torch.relu(self.pool(X))\n","        X = torch.flatten(X, 1)\n","        X = self.fc1(X)\n","        X = self.dropout(X)\n","        X = self.fc2(X)\n","        return X\n","\n"],"metadata":{"id":"5hUN_CX2uci7","executionInfo":{"status":"ok","timestamp":1750208621077,"user_tz":-330,"elapsed":620,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def metric(truevalue, predictedvalue):\n","    print(f\"Accuracy : {accuracy_score(truevalue, predictedvalue)}\")\n","    print(f\"Precision : {precision_score(truevalue, predictedvalue, average='macro')}\")\n","    print(f\"Recall : {recall_score(truevalue, predictedvalue, average='macro')}\")\n","    print(f\"F1 Score: {f1_score(truevalue, predictedvalue, average='macro')}\")\n"],"metadata":{"id":"dpLlWB8AupBa","executionInfo":{"status":"ok","timestamp":1750208621103,"user_tz":-330,"elapsed":9,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print('Training my data')\n","metric(train_true, train_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYids4_Sut97","executionInfo":{"status":"ok","timestamp":1750208621208,"user_tz":-330,"elapsed":102,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"ac016fc9-06eb-4aed-eea6-32616274872d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training my data\n","Accuracy : 0.9403658578916311\n","Precision : 0.9344991202744869\n","Recall : 0.9336922869181946\n","F1 Score: 0.9340919365186638\n"]}]},{"cell_type":"code","source":["import os\n","print(os.listdir('/content/'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72hEnWg5lB5H","executionInfo":{"status":"ok","timestamp":1750208885789,"user_tz":-330,"elapsed":25,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"cb84f240-d09c-4f20-eed7-8b4b7b9e2bdb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['.config', 'drive', 'sample_data']\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"1b8VR92qldQn","executionInfo":{"status":"ok","timestamp":1750209914975,"user_tz":-330,"elapsed":33569,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"07f4cffa-a53f-40bf-f96d-679c0572f496"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6030ecac-76e3-4c79-b062-028fa2c592e6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6030ecac-76e3-4c79-b062-028fa2c592e6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Emotion_model (4).pth to Emotion_model (4).pth\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAPeeaKnnRBR","executionInfo":{"status":"ok","timestamp":1750209473615,"user_tz":-330,"elapsed":3748,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"58a75caa-7f29-4872-ef52-e4dd3811b3ac"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/Emotion_model_weights.pth')"],"metadata":{"id":"zggE3uZLpV-r","executionInfo":{"status":"ok","timestamp":1750210014593,"user_tz":-330,"elapsed":37,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["torch.save(model, '/content/Emotion_model_full.pth')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"FcVtxh3-qBHS","executionInfo":{"status":"error","timestamp":1750210212798,"user_tz":-330,"elapsed":97,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"fbe88250-43db-4df9-c729-598b3b07d4ac"},"execution_count":20,"outputs":[{"output_type":"error","ename":"PicklingError","evalue":"Can't pickle <class '__main__.EmotionClassifier'>: it's not the same object as __main__.EmotionClassifier","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-1901341724>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/Emotion_model_full.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.EmotionClassifier'>: it's not the same object as __main__.EmotionClassifier"]}]},{"cell_type":"code","source":["model = torch.load('/content/emotion_model.pth', weights_only=False)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"meFTwX6VvK2U","executionInfo":{"status":"error","timestamp":1750210029719,"user_tz":-330,"elapsed":72,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"98c0150a-e762-417c-897e-f762b24f6acd"},"execution_count":19,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/emotion_model.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-3102356893>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/emotion_model.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/emotion_model.pth'"]}]},{"cell_type":"code","source":["!pip install pyngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JIZYFSVnrAuB","executionInfo":{"status":"ok","timestamp":1750210460911,"user_tz":-330,"elapsed":9737,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"84cd2c8d-be86-406b-9e36-c9b5d2bc5ddc"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.11\n"]}]},{"cell_type":"code","source":["!npm install localtunnel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qRGDMVvrHAy","executionInfo":{"status":"ok","timestamp":1750210541900,"user_tz":-330,"elapsed":2619,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"6fd5c0ce-9081-4940-be7d-44378fb3e52b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n","added 22 packages in 2s\n","\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n","\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K3 packages are looking for funding\n","\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K  run `npm fund` for details\n","\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K"]}]},{"cell_type":"code","source":["!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9XiEvRorPXI","outputId":"c7e81bc4-b729-47ce-ecc3-438912387a9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34.106.71.41\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://witty-apples-travel.loca.lt\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","class EmotionClassifier(nn.Module):\n","    def __init__(self):\n","        super(EmotionClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128 * 1 * 1, 256)\n","        self.fc2 = nn.Linear(256, 7)\n","\n","    def forward(self, X):\n","        X = self.conv1(X)\n","        X = self.conv2(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv3(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv4(X)\n","        X = torch.relu(self.pool(X))\n","        X = self.conv5(X)\n","        X = torch.relu(self.pool(X))\n","        X = torch.flatten(X, 1)\n","        X = self.fc1(X)\n","        X = self.dropout(X)\n","        X = self.fc2(X)\n","        return X\n","\n","# Load the full trained model\n","model = torch.load(\"/content/drive/MyDrive/Emotion_model.pth\", map_location=\"cpu\",weights_only=False)\n","model.eval()\n","\n","# Class label mapping (edit if different for your dataset)\n","class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n","\n","# Define image transforms\n","transform = transforms.Compose([\n","    transforms.Resize((48, 48)),  # Resize to match training input\n","    transforms.ToTensor(),\n","])\n","\n","# Streamlit app\n","st.title(\"Emotion Detection from Image\")\n","\n","uploaded_file = st.file_uploader(\"Upload a face image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n","\n","if uploaded_file is not None:\n","    image = Image.open(uploaded_file).convert(\"RGB\")\n","    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n","\n","    # Preprocess and predict\n","    img_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n","\n","    with torch.no_grad():\n","        outputs = model(img_tensor)\n","        _, predicted = torch.max(outputs, 1)\n","        predicted_label = class_names[predicted.item()]\n","\n","    st.markdown(f\"### Predicted Emotion: `{predicted_label}`\")\n"],"metadata":{"id":"uLfJKhPzrnyh","executionInfo":{"status":"ok","timestamp":1750210612409,"user_tz":-330,"elapsed":18,"user":{"displayName":"dhanamenee veeramani","userId":"06647466903514544338"}},"outputId":"3bd739e5-6be6-4144-ba0d-fe7832b6df30","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}]}]}